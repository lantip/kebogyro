{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udcda Kebogyro Docs","text":"<p>Welcome to the official documentation for Kebogyro \u2014 your async-first toolkit for LLM orchestration and tool-calling.</p>"},{"location":"#index","title":"\ud83d\udcd6 Index","text":"<ul> <li>Overview</li> <li>Getting Started</li> <li>LLMClientWrapper</li> <li>BBServerMCPClient</li> <li>create_agent</li> <li>Caching</li> <li>Extending Providers</li> <li>Tool Interface (SimpleTool)</li> <li>Troubleshooting</li> </ul>"},{"location":"agent_executor/","title":"\ud83c\udfad Agent Executor","text":"<p>Use <code>create_agent()</code> to wire up your LLM, tools, and optional MCP connection into a single callable agent.</p>"},{"location":"agent_executor/#function-signature","title":"\ud83d\udd27 Function Signature","text":"<pre><code>def create_agent(\n    llm_client: LLMClientWrapper,\n    tools: Optional[List[SimpleTool]],\n    mcp_tools: Optional[BBServerMCPClient],\n    system_prompt: str,\n    stream: bool\n) -&gt; BBAgentExecutor:\n</code></pre>"},{"location":"agent_executor/#example","title":"\ud83d\udce6 Example","text":"<pre><code>from kebogyro.agent_executor import create_agent\n\nagent = create_agent(\n    llm_client=llm,\n    tools=[my_tool],\n    mcp_tools=mcp_client,\n    system_prompt=\"You're a debugging assistant.\",\n    stream=True\n)\n\nresponse = await agent.ainvoke({\"input\": \"Diagnose my Python bug.\"})\n</code></pre>"},{"location":"agent_executor/#notes","title":"\ud83d\udca1 Notes","text":"<ul> <li><code>stream=True</code> enables partial token streaming.</li> <li>All arguments optional except <code>llm_client</code> and <code>system_prompt</code>.</li> <li>Combine LLM + tool logic + remote orchestration in one step.</li> </ul> <p>Next \u2192 Caching</p>"},{"location":"caching/","title":"\ud83e\udde0 Caching in Kebogyro","text":"<p>Caching improves speed and cost efficiency by preventing repeated tool spec fetches and model calls.</p>"},{"location":"caching/#interface-abstractllmcache","title":"\ud83d\udd27 Interface: AbstractLLMCache","text":"<pre><code>class AbstractLLMCache:\n    async def aget_value(self, key: str): ...\n    async def aset_value(self, key: str, value: dict, expiry_seconds: int): ...\n    async def adelete_value(self, key: str): ...\n    async def is_expired(self, key: str, expiry_seconds: int): ...\n</code></pre>"},{"location":"caching/#dual-caching-strategy","title":"\ud83d\udd04 Dual Caching Strategy","text":"<p>Kebogyro caches both:</p> <ul> <li>\ud83d\udd27 Tool metadata (via <code>BBServerMCPClient</code>)</li> <li>\ud83d\udce5 Model responses + tool resolution (via <code>LLMClientWrapper</code>)</li> </ul>"},{"location":"caching/#example-adapter","title":"\ud83d\udce6 Example Adapter","text":"<pre><code>class MyCache(AbstractLLMCache):\n    async def aget_value(self, key): ...\n    async def aset_value(self, key, value, expiry_seconds): ...\n    async def adelete_value(self, key): ...\n    async def is_expired(self, key, expiry_seconds): ...\n</code></pre>"},{"location":"caching/#usage","title":"\ud83e\uddea Usage","text":"<p>Pass to both:</p> <pre><code>llm = LLMClientWrapper(\n    ...,\n    llm_cache=MyCache()\n)\n\nmcp = BBServerMCPClient(\n    ...,\n    cache_adapter=MyCache()\n)\n</code></pre>"},{"location":"caching/#tips","title":"\ud83c\udfaf Tips","text":"<ul> <li>Use Redis or DB cache in production.</li> <li>Avoid caching extremely dynamic responses unless necessary.</li> <li>Align expiry time with tool volatility.</li> </ul> <p>Next \u2192 Extending Providers</p>"},{"location":"getting_started/","title":"\ud83d\ude80 Getting Started with Kebogyro","text":"<p>Welcome! Here's how to get up and running with <code>kebogyro</code> in under 5 minutes.</p>"},{"location":"getting_started/#requirements","title":"\u2705 Requirements","text":"<ul> <li>Python 3.10+</li> <li>A supported LLM provider API key (e.g. OpenRouter, OpenAI)</li> </ul>"},{"location":"getting_started/#install","title":"\ud83d\udce6 Install","text":"<pre><code>pip install ./src/kebogyro\n</code></pre>"},{"location":"getting_started/#minimal-setup","title":"\u2699\ufe0f Minimal Setup","text":""},{"location":"getting_started/#1-define-a-simple-tool","title":"1. Define a Simple Tool","text":"<pre><code>from kebogyro.utils import SimpleTool\n\ndef greet(name: str) -&gt; str:\n    return f\"Hello, {name}!\"\n\ngreet_tool = SimpleTool.from_fn(\n    name=\"greet\",\n    description=\"Greet the user by name\",\n    fn=greet\n)\n</code></pre>"},{"location":"getting_started/#2-create-a-llm-client","title":"2. Create a LLM Client","text":"<pre><code>from kebogyro.wrapper import LLMClientWrapper\n\nllm = LLMClientWrapper(\n    provider=\"openrouter\",\n    model_name=\"mistralai/mistral-7b-instruct\",\n    model_info={\"api_key\": \"sk-...\"},\n    additional_tools=[greet_tool]\n)\n</code></pre>"},{"location":"getting_started/#3-create-the-agent","title":"3. Create the Agent","text":"<pre><code>from kebogyro.agent_executor import create_agent\n\nagent = create_agent(\n    llm_client=llm,\n    tools=[greet_tool],\n    mcp_tools=None,\n    system_prompt=\"You're a greeting agent.\",\n    stream=False\n)\n</code></pre>"},{"location":"getting_started/#4-run-it","title":"4. Run It","text":"<pre><code>response = await agent.ainvoke({\"input\": \"Greet Lantip.\"})\nprint(response)\n</code></pre>"},{"location":"getting_started/#optional-use-mcp-caching","title":"\ud83e\uddf1 Optional: Use MCP + Caching","text":"<pre><code>from kebogyro.mcp_adapter.client import BBServerMCPClient\nfrom kebogyro.cache import AbstractLLMCache\n\nclass MyCache(AbstractLLMCache):\n    async def aget_value(self, key): ...\n    async def aset_value(self, key, value, expiry_seconds): ...\n    async def adelete_value(self, key): ...\n    async def is_expired(self, key, expiry_seconds): ...\n\nmcp = BBServerMCPClient(\n    connections={\n        \"tools\": {\n            \"url\": \"http://localhost:5000/.../sse\",\n            \"transport\": \"sse\"\n        }\n    },\n    cache_adapter=MyCache()\n)\n</code></pre> <p>Pass <code>mcp_tools=mcp</code> when calling <code>create_agent</code>.</p> <p>Next \u2192 LLMClientWrapper</p>"},{"location":"llm_wrapper/","title":"\ud83e\uddf5 LLMClientWrapper","text":"<p><code>LLMClientWrapper</code> is the main interface for invoking LLMs, managing tool calls, and optionally integrating with a cache or MCP client.</p>"},{"location":"llm_wrapper/#constructor","title":"\ud83d\udd27 Constructor","text":"<pre><code>LLMClientWrapper(\n    provider: str,\n    model_name: str,\n    model_info: dict[str, Any],\n    mcp_client: Optional[MCPServerClient] = None,\n    additional_tools: Optional[list[SimpleTool]] = None,\n    system_prompt: Optional[str] = None,\n    cache_key: str = \"global_tools_cache\",\n    llm_cache: Optional[AbstractLLMCache] = None\n)\n</code></pre>"},{"location":"llm_wrapper/#example","title":"\ud83d\udca1 Example","text":"<pre><code>llm = LLMClientWrapper(\n    provider=\"openrouter\",\n    model_name=\"mistralai/mistral-7b-instruct\",\n    model_info={\"api_key\": \"sk-...\"},\n    system_prompt=\"You are a helpful AI.\",\n    additional_tools=[my_tool],\n    llm_cache=MyCache()\n)\n</code></pre>"},{"location":"llm_wrapper/#features","title":"\ud83d\udd0d Features","text":"<ul> <li>Supports OpenAI-compatible chat endpoints</li> <li>Can combine standard tools and MCP tools</li> <li>Works with or without tools</li> <li>Fully async: use <code>await llm.achat()</code> or <code>await agent.ainvoke()</code></li> </ul>"},{"location":"llm_wrapper/#tool-support","title":"\ud83e\uddea Tool Support","text":"<p>Tools passed to <code>additional_tools</code> will be formatted to OpenAI-style <code>function_call</code> specs. You can dynamically combine with tools from <code>BBServerMCPClient</code>.</p>"},{"location":"llm_wrapper/#notes","title":"\ud83e\udde0 Notes","text":"<ul> <li>Set <code>system_prompt</code> for context-specific system messages</li> <li><code>llm_cache</code> can cache both tool specs and call responses</li> </ul> <p>Next \u2192 BBServerMCPClient</p>"},{"location":"mcp_client/","title":"\ud83d\udd0c BBServerMCPClient","text":"<p>This class connects Kebogyro to remote tool runners using the Model Context Protocol (MCP).</p>"},{"location":"mcp_client/#constructor","title":"\ud83d\udee0 Constructor","text":"<pre><code>BBServerMCPClient(\n    connections: dict[str, Connection],\n    tool_cache_expiration_seconds: int = 3600,\n    cache_adapter: Optional[AbstractLLMCache] = None\n)\n</code></pre>"},{"location":"mcp_client/#transports-supported","title":"\ud83d\udd01 Transports Supported","text":"<ul> <li><code>stdio</code>: for local subprocesses</li> <li><code>sse</code>: for streaming HTTP (ideal for FastAPI bridges)</li> <li><code>http</code>: for non-streamable REST endpoints</li> </ul>"},{"location":"mcp_client/#example","title":"\ud83d\udce6 Example","text":"<pre><code>from kebogyro.mcp_adapter.client import BBServerMCPClient\n\nmcp_client = BBServerMCPClient(\n    connections={\n        # \"workroom_tools\" is a namespace (server_name) \u2014 you can define multiple MCP backends under different keys\n        \"workroom_tools\": {\n            \"url\": \"http://localhost:5000/sse\",\n            \"transport\": \"sse\"\n        },\n        \"finance_tools\": {\n            \"url\": \"http://localhost:5100/sse\",\n            \"transport\": \"sse\"\n        }\n    },\n    cache_adapter=MyCache()\n)\n</code></pre>"},{"location":"mcp_client/#namespaces-cache","title":"\ud83e\udde0 Namespaces &amp; Cache","text":"<p>The keys in <code>connections</code> (like <code>workroom_tools</code>, <code>finance_tools</code>, etc.) serve as namespaces or server names.</p> <p>They are automatically reflected in the internal cache structure to keep tool specs separate and avoid collisions across multiple MCP backends.</p>"},{"location":"mcp_client/#caching","title":"\ud83d\udd12 Caching","text":"<p>You can cache: - Remote tool manifests (specs) per namespace - Response payloads</p>"},{"location":"mcp_client/#usage","title":"\ud83d\udccc Usage","text":"<p>Pass <code>mcp_client</code> to <code>LLMClientWrapper</code>, or directly as <code>mcp_tools</code> to <code>create_agent()</code>.</p> <p>Next \u2192 create_agent</p>"},{"location":"overview/","title":"\ud83e\udde0 Overview","text":"<p>Kebogyro is a fast, async-first orchestration layer designed to make it easy to build LLM-powered applications with real tool-calling capabilities.</p> <p>Inspired by:</p> <ul> <li>\ud83d\udee0 Gyro Gearloose \u2014 a symbol of mechanical cleverness</li> <li>\ud83c\udfb6 Kebogiro \u2014 a Javanese ceremonial gamelan suite</li> </ul> <p>Together: <code>kebogyro</code> combines structure, power, and orchestration \u2014 but for AI agents.</p>"},{"location":"overview/#philosophy","title":"\u2728 Philosophy","text":"<ul> <li>Async-native: All I/O is <code>async def</code>, suitable for modern event loops.</li> <li>Composable: Plug in your own LLM provider, cache backend, or tool logic.</li> <li>OpenAI-compatible: Use OpenAI-style tool specs, but with any provider.</li> <li>No framework lock-in: Integrate with anything \u2014 FastAPI, Flask, Celery, etc.</li> </ul>"},{"location":"overview/#architecture","title":"\ud83e\udde9 Architecture","text":"<ul> <li><code>LLMClientWrapper</code>: Base abstraction over LLM APIs</li> <li><code>SimpleTool</code>: Describe tools with arguments and behavior</li> <li><code>BBServerMCPClient</code>: Protocol-aware adapter to remote tool bridges</li> <li><code>create_agent</code>: Glues it all together (LLM + tools + MCP)</li> <li><code>AbstractLLMCache</code>: Optional caching layer for tool specs/results</li> </ul>"},{"location":"overview/#when-to-use","title":"\ud83d\udce6 When to Use","text":"<p>Kebogyro is perfect for:</p> <ul> <li>Multi-provider LLM routing and orchestration</li> <li>Tool-enabled agents in async web backends</li> <li>LLM interfaces where caching or streaming is required</li> <li>Building OpenAI-compatible tool calling into any environment</li> </ul> <p>If you're building a serious LLM agent system that talks to code, services, or other users \u2014 Kebogyro is your async Swiss Army knife.</p> <p>Next \u2192 Getting Started</p>"},{"location":"providers/","title":"\ud83c\udf10 Extending LLM Providers","text":"<p>Kebogyro supports multiple LLM providers via base URL mapping in <code>config.py</code>.</p>"},{"location":"providers/#config-structure","title":"\ud83e\uddf1 Config Structure","text":"<pre><code>class LLMClientConfig(BaseModel):\n    base_urls: Dict[str, HttpUrl] = Field(\n        default_factory=lambda: {\n            \"openrouter\": \"https://openrouter.ai/api/v1\",\n            \"anthropic\": \"https://api.anthropic.com/v1/\",\n            \"cerebras\": \"https://api.cerebras.ai/v1\",\n            \"groq\": \"https://api.groq.ai/openai/v1\",\n            \"requesty\": \"https://router.requesty.ai/v1\"\n        }\n    )\n    google_default_base_url: HttpUrl = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n</code></pre>"},{"location":"providers/#add-a-custom-provider","title":"\u2795 Add a Custom Provider","text":"<pre><code>config = LLMClientConfig()\nconfig.base_urls[\"my_custom\"] = \"https://my.custom-llm.com/v1\"\n</code></pre>"},{"location":"providers/#usage","title":"\u2705 Usage","text":"<p>Then instantiate your wrapper with:</p> <pre><code>llm = LLMClientWrapper(\n    provider=\"my_custom\",\n    model_name=\"awesome-model\",\n    model_info={\"api_key\": \"...\"}\n)\n</code></pre>"},{"location":"providers/#notes","title":"\ud83d\udca1 Notes","text":"<ul> <li>Kebogyro assumes OpenAI-style endpoints unless extended.</li> <li>Custom adapters may be required for non-OpenAI-compatible APIs.</li> </ul> <p>Next \u2192 Tool Interface</p>"},{"location":"tools/","title":"\ud83d\udee0 Tool Interface (SimpleTool)","text":"<p>Kebogyro tools follow the OpenAI function-call format but use a simplified internal schema.</p>"},{"location":"tools/#define-a-tool","title":"\ud83e\uddf1 Define a Tool","text":"<pre><code>from kebogyro.utils import SimpleTool\n\ndef greet(name: str) -&gt; str:\n    return f\"Hello, {name}!\"\n\ntool = SimpleTool.from_fn(\n    name=\"greet\",\n    description=\"Greets the user by name.\",\n    fn=greet\n)\n</code></pre>"},{"location":"tools/#spec-conversion","title":"\ud83d\udd01 Spec Conversion","text":"<p><code>SimpleTool</code> auto-generates the correct JSON schema used by OpenAI-like models.</p>"},{"location":"tools/#best-practices","title":"\ud83e\udde0 Best Practices","text":"<ul> <li>Use type annotations!</li> <li>Keep descriptions concise</li> <li>Avoid global state</li> </ul>"},{"location":"tools/#usage","title":"\ud83d\udccc Usage","text":"<pre><code>llm = LLMClientWrapper(\n    ...,\n    additional_tools=[tool]\n)\n\nagent = create_agent(\n    ..., tools=[tool]\n)\n</code></pre>"},{"location":"tools/#advanced","title":"\ud83d\udd2c Advanced","text":"<p>You may define tools using Pydantic models manually for finer control.</p> <p>Next \u2192 Troubleshooting</p>"},{"location":"troubleshooting/","title":"\ud83e\uddef Troubleshooting Kebogyro","text":"<p>Common issues and how to resolve them.</p>"},{"location":"troubleshooting/#tools-not-calling","title":"\u2753 Tools not calling","text":"<ul> <li>\u2705 Make sure your <code>SimpleTool</code> is correctly passed to both <code>LLMClientWrapper</code> and <code>create_agent()</code></li> <li>\ud83d\udd0d Check that your function has type annotations</li> <li>\ud83e\uddea Add <code>print()</code> in the tool function to debug</li> </ul>"},{"location":"troubleshooting/#mcp-tool-not-resolving","title":"\ud83d\udd0c MCP tool not resolving","text":"<ul> <li>\u2705 Ensure the <code>BBServerMCPClient</code> connection URL is reachable</li> <li>\u2757 Confirm the tool bridge backend supports the correct transport (sse/http)</li> <li>\ud83d\udd01 Try restarting the remote tool bridge service</li> </ul>"},{"location":"troubleshooting/#async-issues","title":"\ud83e\uddf5 Async issues","text":"<ul> <li>\ud83d\udd04 All functions should be awaited \u2014 use <code>await agent.ainvoke(...)</code></li> <li>\ud83e\udde0 Make sure your event loop isn\u2019t blocked (e.g. use <code>asyncio.run()</code> in CLI)</li> </ul>"},{"location":"troubleshooting/#debugging-tips","title":"\ud83e\uddf0 Debugging tips","text":"<ul> <li> <p>Use <code>print()</code> or <code>logging</code> in:</p> </li> <li> <p><code>SimpleTool</code></p> </li> <li>Tool function itself</li> <li>MCP adapter</li> <li>Temporarily disable <code>llm_cache</code> to isolate bugs</li> </ul>"},{"location":"troubleshooting/#still-stuck","title":"\ud83d\udedf Still stuck?","text":"<p>Open a GitHub issue or start a discussion. PRs with fixes are always welcome!</p>"}]}